{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "Many machine learning problems involve thousands or even millions of features for each training instance. Not only do all these features make training extremely slow, but they can also make it much harder to find a good solution, as we will see. This problem is often referred to as the *curse of dimensionality*.\n",
    "\n",
    "It is possible to reduce the number of features considerably.\n",
    "\n",
    "For example, consider the MNIST images: the pixels on the image borders are almost always white, so you could completely drop these pixels fromt the training set without losing much information. Additionally, two neighboring \n",
    "pixels are often highly correlated: if you merge them into a single pixel, you will not lose much information.\n",
    "\n",
    "There are two main approaches to dimensionality reduction - *projection* and *Manifold Learning*.\n",
    "\n",
    "The three most popular dimensionality reduction techniques are: *PCA, Kenel PCA* and *LLE*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "![tcofd.jpg](images/tcofd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
